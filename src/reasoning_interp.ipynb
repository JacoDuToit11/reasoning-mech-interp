{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformer_lens jaxtyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens import patching\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "def delimiter_attention(model, prompts, delimiter=\" because\"):\n",
    "    \"\"\"\n",
    "    Computes and plots the proportion of attention directed to a given delimiter token.\n",
    "    \n",
    "    This function analyzes how much attention each head in the transformer pays to delimiter\n",
    "    words like 'because' or 'so', creating a heatmap visualization of the attention patterns.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        prompts: List of strings to be analyzed\n",
    "        delimiter: Delimiter to focus attention on, e.g., ' because' or ' so'\n",
    "    \"\"\"\n",
    "    print(f\"\\nAnalyzing attention patterns for delimiter: '{delimiter}'\")\n",
    "    n_layers = model.cfg.n_layers\n",
    "    n_heads = model.cfg.n_heads\n",
    "\n",
    "    attn_prop_delim_all_prompts = t.zeros(n_layers, n_heads)\n",
    "\n",
    "    for prompt in prompts:\n",
    "        tokens = model.to_tokens(prompt)\n",
    "        logits, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "        str_tokens = model.to_str_tokens(prompt)\n",
    "\n",
    "        if delimiter not in str_tokens:\n",
    "            print(f\"Prompt '{prompt}' does not contain delimiter '{delimiter}'.\")\n",
    "            continue\n",
    "\n",
    "        delim_idx = str_tokens.index(delimiter)\n",
    "        for layer in range(n_layers):\n",
    "            attn = cache[\"pattern\", layer]\n",
    "            delim_sum = attn[:, :, delim_idx].sum(dim=1)\n",
    "            total_sum = attn.sum(dim=(1, 2))\n",
    "            proportion = delim_sum / total_sum\n",
    "            attn_prop_delim_all_prompts[layer] += proportion.cpu()\n",
    "\n",
    "    attn_prop_delim_all_prompts /= len(prompts)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(attn_prop_delim_all_prompts.numpy(), cmap=\"viridis\")\n",
    "    plt.title(f\"Proportion of Attention to '{delimiter.strip()}'\")\n",
    "    plt.xlabel(\"Heads\")\n",
    "    plt.ylabel(\"Layers\")\n",
    "    plt.savefig(f\"../results/{model_name}/attention_map_delim_{delimiter.strip()}.png\")\n",
    "\n",
    "def analyze_delimiter_attention(model):\n",
    "    \"\"\"\n",
    "    Runs delimiter attention analysis on two sets of prompts:\n",
    "    1. Prompts containing 'because'\n",
    "    2. Prompts containing 'so'\n",
    "    \n",
    "    Creates heatmaps showing how different attention heads focus on these words.\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model to analyze\n",
    "    \"\"\"\n",
    "    print(\"Starting delimiter attention analysis...\")\n",
    "    prompts = [\n",
    "        'Alice went to the park because she wanted to find a treasure.',\n",
    "        'Alice plays guitar because she enjoys strumming melodies.',\n",
    "        'Alice went to Paris because Paris is a good place for art.'\n",
    "    ]\n",
    "    delimiter_attention(model, prompts, delimiter=\" because\")\n",
    "\n",
    "    prompts = [\n",
    "        'Bob and Chris got work to do so they are eager to explore.',\n",
    "        'Bob and Chris made a cake so they are excited and happy.'\n",
    "    ]\n",
    "    delimiter_attention(model, prompts, delimiter=\" so\")\n",
    "\n",
    "def cause_effect_attention(model, prompts, delimiter, direction):\n",
    "    \"\"\"\n",
    "    Measures attention patterns between cause and effect parts of sentences.\n",
    "    \n",
    "    This function analyzes how attention flows between the cause and effect portions\n",
    "    of sentences, separated by delimiters like 'because' or 'so'. It can analyze\n",
    "    attention in both directions.\n",
    "    \n",
    "    Args:\n",
    "        model: model\n",
    "        prompts: List of strings to analyze\n",
    "        delimiter: Token that separates cause from effect (e.g. \" because\", \" so\")\n",
    "        direction: \"cause->effect\" or \"effect->cause\"\n",
    "    Returns:\n",
    "        A (layers x heads) tensor of average attention proportions\n",
    "    \"\"\"\n",
    "    print(f\"\\nAnalyzing {direction} attention patterns for delimiter: '{delimiter}'\")\n",
    "    n_layers = model.cfg.n_layers\n",
    "    n_heads = model.cfg.n_heads\n",
    "    attn_prop_all_prompts = t.zeros(n_layers, n_heads)\n",
    "\n",
    "    for prompt in prompts:\n",
    "        tokens = model.to_tokens(prompt)\n",
    "        logits, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "        str_tokens = model.to_str_tokens(prompt)\n",
    "\n",
    "        if delimiter not in str_tokens:\n",
    "            continue\n",
    "\n",
    "        delim_idx = str_tokens.index(delimiter)\n",
    "\n",
    "        if direction == \"effect->cause\":\n",
    "            cause_idxs = range(1, delim_idx)\n",
    "            effect_idxs = range(delim_idx + 1, len(str_tokens))\n",
    "        elif direction == \"cause->effect\":\n",
    "            cause_idxs = range(delim_idx + 1, len(str_tokens))\n",
    "            effect_idxs = range(1, delim_idx)\n",
    "        else:\n",
    "            raise ValueError(\"direction must be either 'cause->effect' or 'effect->cause'\")\n",
    "\n",
    "        for layer in range(n_layers):\n",
    "            attn = cache[\"pattern\", layer]\n",
    "\n",
    "            if direction == \"cause->effect\":\n",
    "                relevant_attn = attn[:, cause_idxs][:, :, effect_idxs]\n",
    "            elif direction == \"effect->cause\":\n",
    "                relevant_attn = attn[:, effect_idxs][:, :, cause_idxs]\n",
    "            else:\n",
    "                raise ValueError(\"direction must be either 'cause->effect' or 'effect->cause'\")\n",
    "\n",
    "            sum_relevant = relevant_attn.sum(dim=(1, 2)).cpu()\n",
    "            total_sum = attn.sum(dim=(1, 2)).cpu()\n",
    "            attn_prop_all_prompts[layer] += sum_relevant / total_sum\n",
    "\n",
    "    attn_prop_all_prompts /= len(prompts)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(attn_prop_all_prompts.numpy(), cmap=\"viridis\")\n",
    "    if direction == 'cause->effect':\n",
    "        plt.title(\"Proportion of Cause-to-Effect Attention\")\n",
    "    else:\n",
    "        plt.title(\"Proportion of Effect-to-Cause Attention\")\n",
    "    plt.xlabel(\"Heads\")\n",
    "    plt.ylabel(\"Layers\")\n",
    "    plt.savefig(f\"../results/{model_name}/attention_map_cause_effect_{delimiter.strip()}.png\")\n",
    "\n",
    "def analyze_causal_attention(model):\n",
    "    prompts = [\n",
    "        \"Alice went to the park because she wanted to find a treasure.\",\n",
    "        \"Alice plays guitar because she enjoys strumming melodies.\",\n",
    "        \"Alice went to Paris because Paris is a good place for art.\"\n",
    "    ]\n",
    "\n",
    "    cause_effect_attention(model, prompts,\n",
    "                                        delimiter=\" because\",\n",
    "                                        direction=\"effect->cause\")\n",
    "\n",
    "    prompts = [\n",
    "        \"Alice went to the craft fair so she could buy handmade gifts.\",\n",
    "        \"Alice practiced daily so she would master the guitar.\"\n",
    "    ]\n",
    "\n",
    "    cause_effect_attention(model, prompts,\n",
    "                                        delimiter=\" so\",\n",
    "                                        direction=\"cause->effect\")\n",
    "    \n",
    "def logits_to_ave_logit_diff(\n",
    "    logits: Float[Tensor, \"batch seq d_vocab\"],\n",
    "    answer_tokens: Float[Tensor, \"batch 2\"],\n",
    "    per_prompt: bool = False,\n",
    ") -> Float[Tensor, \"*batch\"]:\n",
    "    \"\"\"\n",
    "    Returns logit difference between the correct and incorrect answer.\n",
    "\n",
    "    If per_prompt=True, return the array of differences rather than the average.\n",
    "    \"\"\"\n",
    "    device = logits.device\n",
    "    answer_tokens = answer_tokens.to(device)\n",
    "    # Only the final logits are relevant for the answer\n",
    "    final_logits: Float[Tensor, \"batch d_vocab\"] = logits[:, -1, :]\n",
    "    # Get the logits corresponding to the indirect object / subject tokens respectively\n",
    "    answer_logits: Float[Tensor, \"batch 2\"] = final_logits.gather(dim=-1, index=answer_tokens)\n",
    "    # Find logit difference\n",
    "    correct_logits, incorrect_logits = answer_logits.unbind(dim=-1)\n",
    "    answer_logit_diff = correct_logits - incorrect_logits\n",
    "    return answer_logit_diff if per_prompt else answer_logit_diff.mean()\n",
    "\n",
    "def activation_patching(model, category, template_title, dataset):\n",
    "    \"\"\"\n",
    "    Performs activation patching analysis on the model to understand how different\n",
    "    components contribute to the model's predictions.\n",
    "    \n",
    "    This function creates two types of visualizations:\n",
    "    1. Residual stream activation patching across layers and positions\n",
    "    2. Attention head output patching across heads and positions\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model to analyze\n",
    "        category: String identifier for the current analysis\n",
    "        template_title: String identifier for the current template\n",
    "        dataset: Dictionary containing clean_tokens, corrupted_tokens, and answers\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\nPerforming activation patching analysis for template: {template_title}\")\n",
    "    clean_tokens = dataset['clean_tokens'].to(device)\n",
    "    corrupted_tokens = dataset['corrupted_tokens'].to(device)\n",
    "    answers = dataset['answers']\n",
    "\n",
    "    answer_tokens = t.concat([\n",
    "        model.to_tokens(answers, prepend_bos=False).T for answers in answers\n",
    "    ]).to(device)\n",
    "\n",
    "    clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "    corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "    clean_logit_diff = logits_to_ave_logit_diff(clean_logits, answer_tokens)\n",
    "    print(f\"Clean logit diff: {clean_logit_diff:.4f}\")\n",
    "\n",
    "    corrupted_logit_diff = logits_to_ave_logit_diff(corrupted_logits, answer_tokens)\n",
    "    print(f\"Corrupted logit diff: {corrupted_logit_diff:.4f}\")\n",
    "    \n",
    "    def logit_diff_metric(\n",
    "        logits: Float[Tensor, \"batch seq d_vocab\"],\n",
    "        answer_tokens: Float[Tensor, \"batch 2\"] = answer_tokens,\n",
    "        corrupted_logit_diff: float = corrupted_logit_diff,\n",
    "        clean_logit_diff: float = clean_logit_diff,\n",
    "    ) -> Float[Tensor, \"\"]:\n",
    "        \"\"\"\n",
    "        Linear function of logit diff, calibrated so that it equals 0 when performance is same as on corrupted input, and 1\n",
    "        when performance is same as on clean input.\n",
    "        \"\"\"\n",
    "        patched_logit_diff = logits_to_ave_logit_diff(logits, answer_tokens)\n",
    "        return (patched_logit_diff - corrupted_logit_diff) / (clean_logit_diff - corrupted_logit_diff)\n",
    "\n",
    "    if not skip_resid_pre:\n",
    "    \n",
    "        act_patch_resid_pre = patching.get_act_patch_resid_pre(\n",
    "            model=model,\n",
    "            corrupted_tokens=corrupted_tokens,\n",
    "            clean_cache=clean_cache,\n",
    "            patching_metric=logit_diff_metric\n",
    "        )\n",
    "\n",
    "        labels = [f\"{tok} {i}\" for i, tok in enumerate(model.to_str_tokens(clean_tokens[0]))]\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(\n",
    "            act_patch_resid_pre.cpu(),\n",
    "            cmap=\"viridis\",\n",
    "            aspect=\"auto\"\n",
    "        )\n",
    "\n",
    "        plt.colorbar(label=\"Activation Values\")\n",
    "        plt.xlabel(\"Position\")\n",
    "        plt.ylabel(\"Layer\")\n",
    "        plt.title(\"Resid_Pre Activation Patching\")\n",
    "\n",
    "        if 'labels' in locals() and labels is not None:\n",
    "            plt.xticks(ticks=np.arange(len(labels)), labels=labels, rotation=45)\n",
    "\n",
    "        plt.savefig(f\"../results/{model_name}/{category}/activation_patching_per_block_{template_title}.png\")\n",
    "\n",
    "    act_patch_attn_head_out_all_pos = patching.get_act_patch_attn_head_out_all_pos(\n",
    "        model,\n",
    "        corrupted_tokens,\n",
    "        clean_cache,\n",
    "        logit_diff_metric\n",
    "    )\n",
    "\n",
    "    # Get the maximum absolute value from the data\n",
    "    max_abs_val = abs(act_patch_attn_head_out_all_pos.cpu()).max()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(\n",
    "        act_patch_attn_head_out_all_pos.cpu(),\n",
    "        cmap=\"RdBu\",\n",
    "        aspect=\"auto\",\n",
    "        vmin=-max_abs_val,  # Symmetric negative bound\n",
    "        vmax=max_abs_val    # Symmetric positive bound\n",
    "    )\n",
    "    plt.colorbar(label=\"Value\")\n",
    "    plt.xlabel(\"Head\")\n",
    "    plt.ylabel(\"Layer\")\n",
    "    plt.title(\"Attention Head Outputs Heatmap\")\n",
    "    os.makedirs(os.path.join(directory_name, category), exist_ok=True)\n",
    "    plt.savefig(f\"{directory_name}/{category}/activation_patching_attn_head_out_all_pos_{template_title}.png\")\n",
    "\n",
    "    # Return the attention head outputs for averaging\n",
    "    return act_patch_attn_head_out_all_pos\n",
    "\n",
    "def create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template):\n",
    "    dataset = {\n",
    "        'clean_tokens': model.to_tokens([\n",
    "            base_template.format(token)\n",
    "            for token, _ in clean_pairs\n",
    "        ]),\n",
    "        'corrupted_tokens': model.to_tokens([\n",
    "            base_template.format(token)\n",
    "            for token, _ in corrupted_pairs\n",
    "        ]),\n",
    "        'answers': [\n",
    "            (f\" {clean[1]}\", f\" {corrupt[1]}\")\n",
    "            for clean, corrupt in zip(clean_pairs, corrupted_pairs)\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def activation_patching_paper_templates_analysis(model):\n",
    "    \"\"\"\n",
    "    Runs activation patching experiments on various templates of prompts from paper.\n",
    "    \n",
    "    Analyzes different types of causal relationships using templates:\n",
    "    - ALB (Action Location Because)\n",
    "    - ALS (Action Location So)\n",
    "    - ALS-2 (Alternative Action Location So)\n",
    "    - AOS (Action Object So)\n",
    "    - AOB (Action Object Because)\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model to analyze\n",
    "    \"\"\"\n",
    "    print(\"Starting activation patching analysis across semantic templates...\")\n",
    "    category = \"paper_templates\"\n",
    "    \n",
    "    # Store all attention head outputs\n",
    "    all_attn_head_outputs = []\n",
    "    \n",
    "    # Template: \"John had to [ACTION] because he is going to the [LOCATION]\"\n",
    "    template_title = \"ALB\"\n",
    "    base_template = \"John had to {} because he is going to the\"\n",
    "    \n",
    "    clean_pairs = [\n",
    "        (\"dress\", \"show\"),\n",
    "        (\"shave\", \"meeting\"),\n",
    "        (\"study\", \"exam\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"work\", \"office\"),\n",
    "        (\"train\", \"gym\"), \n",
    "        (\"pack\", \"airport\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Template: \"Jane will [ACTION] it because John is getting the [OBJECT]\"\n",
    "    template_title = \"AOB\"\n",
    "    base_template = \"Jane will {} it because John is getting the\"\n",
    "    clean_pairs = [\n",
    "        (\"read\", \"book\"),\n",
    "        (\"eat\", \"food\"),\n",
    "        (\"slice\", \"bread\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"heat\", \"pot\"),\n",
    "        (\"sketch\", \"pencil\"),\n",
    "        (\"wash\", \"dish\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Template: \"Mary went to the [LOCATION] so she wants to [ACTION]\"\n",
    "    template_title = \"ALS\"\n",
    "    base_template = \"Mary went to the {} so she wants to\"\n",
    "    clean_pairs = [\n",
    "        (\"store\", \"shop\"),\n",
    "        (\"church\", \"pray\"),\n",
    "        (\"airport\", \"fly\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"exam\", \"write\"),\n",
    "        (\"gym\", \"exercise\"),\n",
    "        (\"library\", \"read\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Template: \"Nadia will be at the [LOCATION] so she will [ACTION]\"\n",
    "    template_title = \"ALS-2\"\n",
    "    base_template = \"Nadia will be at the {} so she will\"\n",
    "    clean_pairs = [\n",
    "        (\"beach\", \"swim\"),\n",
    "        (\"church\", \"pray\"),\n",
    "        (\"airport\", \"fly\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"library\", \"read\"),\n",
    "        (\"gym\", \"exercise\"),\n",
    "        (\"store\", \"shop\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Template: \"Sara wanted to [ACTION] so Mark decided to get the [OBJECT]\"\n",
    "    template_title = \"AOS\"\n",
    "    base_template = \"Sara wanted to {} so Mark decided to get the\"\n",
    "    clean_pairs = [\n",
    "        (\"study\", \"book\"),\n",
    "        (\"paint\", \"canvas\"),\n",
    "        (\"write\", \"pen\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"wash\", \"dish\"),\n",
    "        (\"sketch\", \"pencil\"),\n",
    "        (\"cook\", \"pot\"),\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Calculate and save the average\n",
    "    avg_attn_head_out = t.stack(all_attn_head_outputs).mean(dim=0)\n",
    "    \n",
    "    # Plot and save the average\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    max_abs_val = abs(avg_attn_head_out.cpu()).max()\n",
    "    plt.imshow(\n",
    "        avg_attn_head_out.cpu(),\n",
    "        cmap=\"RdBu\",\n",
    "        aspect=\"auto\",\n",
    "        vmin=-max_abs_val,\n",
    "        vmax=max_abs_val\n",
    "    )\n",
    "    plt.colorbar(label=\"Value\")\n",
    "    plt.xlabel(\"Head\")\n",
    "    plt.ylabel(\"Layer\")\n",
    "    plt.title(\"Average Attention Head Outputs Across Semantic Templates\")\n",
    "    os.makedirs(os.path.join(directory_name, category), exist_ok=True)\n",
    "    plt.savefig(f\"{directory_name}/{category}/activation_patching_attn_head_out_avg.png\")\n",
    "    plt.close()\n",
    "\n",
    "def activation_patching_mathematical_analysis(model):\n",
    "    \"\"\"\n",
    "    Runs activation patching experiments on mathematical reasoning templates.\n",
    "    \n",
    "    Analyzes different types of mathematical relationships using templates:\n",
    "    - MAB (Mathematical Action Because)\n",
    "    - MAB-2 (Alternative Mathematical Action Because)\n",
    "    - MPS (Mathematical Progressive So)\n",
    "    - MPS-2 (Alternative Mathematical Progressive So)\n",
    "    - MRS (Mathematical Requirement So)\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model to analyze\n",
    "    \"\"\"\n",
    "    print(\"Starting activation patching analysis across mathematical templates...\")\n",
    "    category = \"mathematical\"\n",
    "    all_attn_head_outputs = []\n",
    "\n",
    "    # Template: \"John had [X] apples but now has 8 because Mary gave him\"\n",
    "    template_title = \"MATH-B-1\"\n",
    "    base_template = \"John had {} apples but now has 10 because Mary gave him\"\n",
    "    clean_pairs = [\n",
    "        (\"1\", \"9\"),\n",
    "        (\"8\", \"2\"),\n",
    "        (\"6\", \"4\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"3\", \"7\"),\n",
    "        (\"5\", \"5\"),\n",
    "        (\"0\", \"10\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Template: \"Jane needs [X] apples because she already has 6 and wants a total of\"\n",
    "    template_title = \"MATH-B-2\"\n",
    "    base_template = \"Jane needs {} apples because she already has 6 and wants a total of\"\n",
    "    clean_pairs = [\n",
    "        (\"4\", \"10\"),\n",
    "        (\"25\", \"31\"),\n",
    "        (\"116\", \"122\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"3\", \"9\"),\n",
    "        (\"27\", \"33\"),\n",
    "        (\"63\", \"69\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Template: \"Mary got [X] oranges so now she has 8 after starting with\"\n",
    "    template_title = \"MATH-S-1\"\n",
    "    base_template = \"Mary got {} oranges so now she has 8 after starting with\"\n",
    "    clean_pairs = [\n",
    "        (\"3\", \"5\"), \n",
    "        (\"4\", \"4\"),  \n",
    "        (\"2\", \"6\")  \n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"5\", \"3\"),  \n",
    "        (\"6\", \"2\"),  \n",
    "        (\"1\", \"7\")  \n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Template: \"Nadia shared [X] bananas so she only has 2 after starting with\"\n",
    "    template_title = \"MATH-S-2\"\n",
    "    base_template = \"Nadia shared {} bananas so she only has 2 after starting with\"\n",
    "    clean_pairs = [\n",
    "        (\"3\", \"5\"),\n",
    "        (\"4\", \"6\"),\n",
    "        (\"118\", \"120\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"2\", \"4\"),\n",
    "        (\"19\", \"21\"),\n",
    "        (\"7\", \"9\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Template: \"Sarah needed [X] pencils so she could complete her set of 10 after starting with\"\n",
    "    template_title = \"MATH-S-3\"\n",
    "    base_template = \"Sarah needed {} pencils so she could complete her set of 10 after starting with\"\n",
    "    clean_pairs = [\n",
    "        (\"3\", \"7\"),\n",
    "        (\"5\", \"5\"),\n",
    "        (\"4\", \"6\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"4\", \"6\"),\n",
    "        (\"2\", \"8\"),\n",
    "        (\"1\", \"9\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Calculate and save the average\n",
    "    avg_attn_head_out = t.stack(all_attn_head_outputs).mean(dim=0)\n",
    "    \n",
    "    # Plot and save the average\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    max_abs_val = abs(avg_attn_head_out.cpu()).max()\n",
    "    plt.imshow(\n",
    "        avg_attn_head_out.cpu(),\n",
    "        cmap=\"RdBu\",\n",
    "        aspect=\"auto\",\n",
    "        vmin=-max_abs_val,\n",
    "        vmax=max_abs_val\n",
    "    )\n",
    "    plt.colorbar(label=\"Value\")\n",
    "    plt.xlabel(\"Head\")\n",
    "    plt.ylabel(\"Layer\")\n",
    "    plt.title(\"Average Attention Head Outputs Across Mathematical Templates\")\n",
    "    os.makedirs(os.path.join(directory_name, category), exist_ok=True)\n",
    "    plt.savefig(f\"{directory_name}/{category}/activation_patching_attn_head_out_avg.png\")\n",
    "    plt.close()\n",
    "\n",
    "def activation_patching_coding_analysis(model):\n",
    "    \"\"\"\n",
    "    Runs activation patching experiments on simple coding reasoning templates.\n",
    "    \n",
    "    Analyzes different types of programming logic including:\n",
    "    - Variable assignment and values\n",
    "    - Simple conditionals\n",
    "    - Basic loops\n",
    "    - Function return values\n",
    "    - Array/list operations\n",
    "    \n",
    "    Both clean and corrupted pairs represent valid code with correct outputs,\n",
    "    but demonstrate different programming patterns to achieve the results.\n",
    "    \"\"\"\n",
    "    print(\"Starting activation patching analysis across coding reasoning templates...\")\n",
    "    category = \"coding\"\n",
    "    all_attn_head_outputs = []\n",
    "\n",
    "    # Variable Assignment\n",
    "    template_title = \"VARS\"\n",
    "    base_template = \"x = 5\\ny = {}\\nz = x + y\\nz equals\"\n",
    "    clean_pairs = [\n",
    "        (\"3\", \"8\"),    # Adding small numbers\n",
    "        (\"10\", \"15\"),  # Adding larger numbers\n",
    "        (\"0\", \"5\")     # Adding zero\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"15\", \"20\"),  # Different but valid additions\n",
    "        (\"25\", \"30\"),\n",
    "        (\"20\", \"25\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Conditionals\n",
    "    template_title = \"IF\"\n",
    "    base_template = \"x = {}\\nif x < 10:\\n    print('small')\\nelse:\\n    print('big')\\nThe code will print\"\n",
    "    clean_pairs = [\n",
    "        (\"5\", \"small\"),    # Single digit numbers\n",
    "        (\"3\", \"small\"),\n",
    "        (\"2\", \"small\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"15\", \"big\"),     # Double digit numbers\n",
    "        (\"12\", \"big\"),\n",
    "        (\"11\", \"big\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Loops\n",
    "    template_title = \"LOOP\"\n",
    "    base_template = \"total = 0\\nfor i in range({}):\\n    total += 2\\ntotal equals\"\n",
    "    clean_pairs = [\n",
    "        (\"3\", \"6\"),    # Small ranges\n",
    "        (\"4\", \"8\"),\n",
    "        (\"5\", \"10\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"10\", \"20\"),  # Larger ranges\n",
    "        (\"8\", \"16\"),\n",
    "        (\"6\", \"12\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Function Returns\n",
    "    template_title = \"FUNC\"\n",
    "    base_template = \"def func(x):\\n    return x * {}\\n\\nresult = func(2)\\nresult equals\"\n",
    "    clean_pairs = [\n",
    "        (\"3\", \"6\"),    # Multiplication by small numbers\n",
    "        (\"4\", \"8\"),\n",
    "        (\"5\", \"10\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"10\", \"20\"),  # Multiplication by larger numbers\n",
    "        (\"8\", \"16\"),\n",
    "        (\"6\", \"12\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # List Operations\n",
    "    template_title = \"LIST\"\n",
    "    base_template = \"nums = [1, 2, {}]\\nsum = 0\\nfor n in nums:\\n    sum += n\\nsum equals\"\n",
    "    clean_pairs = [\n",
    "        (\"3\", \"6\"),     # Small numbers in list\n",
    "        (\"4\", \"7\"),\n",
    "        (\"5\", \"8\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"10\", \"13\"),   # Larger numbers in list\n",
    "        (\"15\", \"18\"),\n",
    "        (\"20\", \"23\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Calculate and save the average\n",
    "    avg_attn_head_out = t.stack(all_attn_head_outputs).mean(dim=0)\n",
    "    \n",
    "    # Plot and save the average\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    max_abs_val = abs(avg_attn_head_out.cpu()).max()\n",
    "    plt.imshow(\n",
    "        avg_attn_head_out.cpu(),\n",
    "        cmap=\"RdBu\",\n",
    "        aspect=\"auto\",\n",
    "        vmin=-max_abs_val,\n",
    "        vmax=max_abs_val\n",
    "    )\n",
    "    plt.colorbar(label=\"Value\")\n",
    "    plt.xlabel(\"Head\")\n",
    "    plt.ylabel(\"Layer\")\n",
    "    plt.title(\"Average Attention Head Outputs Across Coding Templates\")\n",
    "    plt.savefig(f\"../results/{model_name}/{category}/activation_patching_attn_head_out_avg.png\")\n",
    "    plt.close()\n",
    "\n",
    "def activation_patching_coding_logic_analysis(model):\n",
    "    \"\"\"\n",
    "    Runs activation patching experiments on logical coding templates.\n",
    "    \n",
    "    Analyzes different types of programming logic including:\n",
    "    - String operations\n",
    "    - Boolean logic\n",
    "    - List operations (non-numeric)\n",
    "    - Dictionary lookups\n",
    "    - Control flow\n",
    "    \n",
    "    Both clean and corrupted pairs represent valid code with correct outputs,\n",
    "    but demonstrate different programming patterns to achieve the results.\n",
    "    \"\"\"\n",
    "    print(\"Starting activation patching analysis across logical coding templates...\")\n",
    "    category = \"coding_logic\"\n",
    "    all_attn_head_outputs = []\n",
    "\n",
    "    # String Operations\n",
    "    template_title = \"STRING\"\n",
    "    base_template = \"text = '{}'\\nif text.startswith('a'):\\n    result = 'yes'\\nelse:\\n    result = 'no'\\nresult equals\"\n",
    "    clean_pairs = [\n",
    "        (\"art\", \"yes\"),      # Simple 'a' words\n",
    "        (\"age\", \"yes\"),\n",
    "        (\"air\", \"yes\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"dog\", \"no\"),      # Non-'a' words\n",
    "        (\"cat\", \"no\"),\n",
    "        (\"box\", \"no\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Boolean Logic\n",
    "    template_title = \"BOOL\"\n",
    "    base_template = \"is_sunny = {}\\nis_warm = True\\ncan_swim = is_sunny and is_warm\\ncan_swim equals\"\n",
    "    clean_pairs = [\n",
    "        (\"True\", \"True\"),      # Both conditions true\n",
    "        (\"True\", \"True\"),\n",
    "        (\"True\", \"True\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"False\", \"False\"),    # One condition false\n",
    "        (\"False\", \"False\"),\n",
    "        (\"False\", \"False\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # List Operations (non-numeric)\n",
    "    template_title = \"LIST\"\n",
    "    base_template = \"fruits = ['apple', 'banana', '{}']\\nif 'apple' in fruits:\\n    result = 'found'\\nelse:\\n    result = 'missing'\\nresult equals\"\n",
    "    clean_pairs = [\n",
    "        (\"orange\", \"found\"),    # Lists with apple\n",
    "        (\"grape\", \"found\"),\n",
    "        (\"mango\", \"found\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"banana\", \"found\"),    # Different lists with apple\n",
    "        (\"kiwi\", \"found\"),\n",
    "        (\"peach\", \"found\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Dictionary Operations\n",
    "    template_title = \"DICT\"\n",
    "    base_template = \"user = {{'name': '{}', 'active': True}}\\nif user['active']:\\n    status = 'online'\\nelse:\\n    status = 'offline'\\nstatus equals\"\n",
    "    clean_pairs = [\n",
    "        (\"Alice\", \"online\"),    # Different active users\n",
    "        (\"Bob\", \"online\"),\n",
    "        (\"Charlie\", \"online\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"David\", \"online\"),    # Other active users\n",
    "        (\"Eve\", \"online\"),\n",
    "        (\"Frank\", \"online\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Control Flow\n",
    "    template_title = \"FLOW\"\n",
    "    base_template = \"status = '{}'\\nif status == 'error':\\n    msg = 'failed'\\nelif status == 'success':\\n    msg = 'passed'\\nelse:\\n    msg = 'unknown'\\nmsg equals\"\n",
    "    clean_pairs = [\n",
    "        (\"error\", \"failed\"),     # Error cases\n",
    "        (\"error\", \"failed\"),\n",
    "        (\"error\", \"failed\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"success\", \"passed\"),   # Success cases\n",
    "        (\"success\", \"passed\"),\n",
    "        (\"success\", \"passed\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Calculate and save the average\n",
    "    avg_attn_head_out = t.stack(all_attn_head_outputs).mean(dim=0)\n",
    "    \n",
    "    # Plot and save the average\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    max_abs_val = abs(avg_attn_head_out.cpu()).max()\n",
    "    plt.imshow(\n",
    "        avg_attn_head_out.cpu(),\n",
    "        cmap=\"RdBu\",\n",
    "        aspect=\"auto\",\n",
    "        vmin=-max_abs_val,\n",
    "        vmax=max_abs_val\n",
    "    )\n",
    "    plt.colorbar(label=\"Value\")\n",
    "    plt.xlabel(\"Head\")\n",
    "    plt.ylabel(\"Layer\")\n",
    "    plt.title(\"Average Attention Head Outputs Across Logical Coding Templates\")\n",
    "    plt.savefig(f\"../results/{model_name}/{category}/activation_patching_attn_head_out_avg.png\")\n",
    "    plt.close()\n",
    "\n",
    "def activation_patching_emotional_analysis(model):\n",
    "    \"\"\"\n",
    "    Runs activation patching experiments on emotional reasoning templates.\n",
    "    \n",
    "    Analyzes different types of emotional relationships using templates:\n",
    "    - EAB (Emotional Action Because)\n",
    "    - ERS (Emotional Response So)\n",
    "    - ESB (Emotional State Because)\n",
    "    - ECS (Emotional Consequence So)\n",
    "    - ECB (Emotional Cause Because)\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model to analyze\n",
    "    \"\"\"\n",
    "    print(\"Starting activation patching analysis across emotional templates...\")\n",
    "    category = \"emotional\"\n",
    "    all_attn_head_outputs = []\n",
    "\n",
    "    # Template: \"Tom [ACTION] because Pete made him feel [EMOTION]\"\n",
    "    template_title = \"EAB\"\n",
    "    base_template = \"Tom {} because Pete made him feel\"\n",
    "    clean_pairs = [\n",
    "        (\"smiled\", \"happy\"),\n",
    "        (\"cried\", \"sad\"),\n",
    "        (\"trembled\", \"scared\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"frowned\", \"angry\"),\n",
    "        (\"laughed\", \"amused\"),\n",
    "        (\"left\", \"bad\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Template: \"Lisa felt [EMOTION] so she decided to [ACTION]\"\n",
    "    template_title = \"ERS\"\n",
    "    base_template = \"Lisa felt {} so she decided to\"\n",
    "    clean_pairs = [\n",
    "        (\"scared\", \"hide\"),\n",
    "        (\"excited\", \"dance\"),\n",
    "        (\"angry\", \"shout\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"sad\", \"cry\"),\n",
    "        (\"jealous\", \"fight\"),\n",
    "        (\"happy\", \"laugh\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Template: \"Emma feels [EMOTION] because her [OBJECT] is [STATE]\"\n",
    "    template_title = \"ESB\"\n",
    "    base_template = \"Emma feels {} because her best friend is\"\n",
    "    clean_pairs = [\n",
    "        (\"lonely\", \"away\"),\n",
    "        (\"proud\", \"successful\"),\n",
    "        (\"worried\", \"sick\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"happy\", \"here\"),\n",
    "        (\"excited\", \"visiting\"),\n",
    "        (\"calm\", \"sleeping\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Template: \"Sarah is [EMOTION] because she got a new [OBJECT]\"\n",
    "    template_title = \"ECB\"\n",
    "    base_template = \"Sarah is {} because she got a new\"\n",
    "    clean_pairs = [\n",
    "        (\"happy\", \"puppy\"),\n",
    "        (\"nervous\", \"job\"),\n",
    "        (\"excited\", \"gift\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"sad\", \"problem\"),\n",
    "        (\"proud\", \"prize\"),\n",
    "        (\"worried\", \"test\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Calculate and save the average\n",
    "    avg_attn_head_out = t.stack(all_attn_head_outputs).mean(dim=0)\n",
    "    \n",
    "    # Plot and save the average\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    max_abs_val = abs(avg_attn_head_out.cpu()).max()\n",
    "    plt.imshow(\n",
    "        avg_attn_head_out.cpu(),\n",
    "        cmap=\"RdBu\",\n",
    "        aspect=\"auto\",\n",
    "        vmin=-max_abs_val,\n",
    "        vmax=max_abs_val\n",
    "    )\n",
    "    plt.colorbar(label=\"Value\")\n",
    "    plt.xlabel(\"Head\")\n",
    "    plt.ylabel(\"Layer\")\n",
    "    plt.title(\"Average Attention Head Outputs Across Emotional Templates\")\n",
    "    os.makedirs(os.path.join(directory_name, category), exist_ok=True)\n",
    "    plt.savefig(f\"{directory_name}/{category}/activation_patching_attn_head_out_avg.png\")\n",
    "    plt.close()\n",
    "\n",
    "def activation_patching_physical_analysis(model):\n",
    "    \"\"\"\n",
    "    Runs activation patching experiments on physical causation templates.\n",
    "    \n",
    "    Analyzes different types of physical cause-effect relationships using templates:\n",
    "    - PCB (Physical Cause Because) - Direct physical causes\n",
    "    - PCS (Physical Consequence So) - Resulting physical states\n",
    "    - PMB (Physical Material Because) - Material properties\n",
    "    - PFB (Physical Force Because) - Force and motion\n",
    "    - PSB (Physical State Because) - Environmental conditions\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model to analyze\n",
    "    \"\"\"\n",
    "    print(\"Starting activation patching analysis across physical templates...\")\n",
    "\n",
    "    # The <object> broke because it \n",
    "    template = \"PCB\"\n",
    "    dataset = {'clean_tokens': model.to_tokens([\n",
    "        'The glass broke because it fell on the',\n",
    "        'The metal bent because it was hit with the',\n",
    "        'The ice melted because it was left in the',\n",
    "        ]),\n",
    "              'corrupted_tokens': model.to_tokens([\n",
    "                  'The glass floated because it fell on the',\n",
    "                  'The metal sparkled because it was hit with the',\n",
    "                  'The ice expanded because it was left in the']),\n",
    "              'answers': [(' concrete', ' carpet'),\n",
    "                        (' hammer', ' feather'),\n",
    "                        (' heat', ' shade')]\n",
    "            }\n",
    "    activation_patching(model, template, dataset)\n",
    "\n",
    "    template = \"PCS\"\n",
    "    dataset = {'clean_tokens': model.to_tokens([\n",
    "        'The metal was hot so it turned',\n",
    "        'The ball was hit so it went',\n",
    "        'The ice was warm so it turned',\n",
    "        ]),\n",
    "              'corrupted_tokens': model.to_tokens([\n",
    "                  'The metal was new so it turned',\n",
    "                  'The ball was old so it went',\n",
    "                  'The ice was cold so it turned']),\n",
    "              'answers': [(' red', ' grey'), (' up', ' down'), (' soft', ' hard')]\n",
    "            }\n",
    "    activation_patching(model, template, dataset)\n",
    "\n",
    "    template = \"PMB\"\n",
    "    dataset = {'clean_tokens': model.to_tokens([\n",
    "        'The cloth tore because it was too',\n",
    "        'The rope broke because it was too',\n",
    "        'The food spoiled because it was too',\n",
    "        ]),\n",
    "              'corrupted_tokens': model.to_tokens([\n",
    "                  'The cloth moved because it was too',\n",
    "                  'The rope stretched because it was too',\n",
    "                  'The food changed because it was too']),\n",
    "              'answers': [(' thin', ' soft'), (' weak', ' long'), (' hot', ' fresh')]\n",
    "            }\n",
    "    activation_patching(model, template, dataset)\n",
    "\n",
    "    template = \"PFB\"\n",
    "    dataset = {'clean_tokens': model.to_tokens([\n",
    "        'The tree fell because the wind was too',\n",
    "        'The car slid because the road was too',\n",
    "        'The boat tipped because the sea was too',\n",
    "        ]),\n",
    "              'corrupted_tokens': model.to_tokens([\n",
    "                  'The tree bent because the wind was too',\n",
    "                  'The car stopped because the road was too',\n",
    "                  'The boat moved because the sea was too']),\n",
    "              'answers': [(' strong', ' weak'), (' wet', ' rough'), (' wild', ' calm')]\n",
    "            }\n",
    "    activation_patching(model, template, dataset)\n",
    "\n",
    "    template = \"PSB\"\n",
    "    dataset = {'clean_tokens': model.to_tokens([\n",
    "        'The steel rusted because the air was too',\n",
    "        'The plant died because the soil was too',\n",
    "        'The food melted because the room was too',\n",
    "        ]),\n",
    "              'corrupted_tokens': model.to_tokens([\n",
    "                  'The steel shone because the air was too',\n",
    "                  'The plant grew because the soil was too',\n",
    "                  'The food froze because the room was too']),\n",
    "              'answers': [(' wet', ' dry'), (' dry', ' rich'), (' hot', ' cold')]\n",
    "            }\n",
    "    activation_patching(model, template, dataset)\n",
    "\n",
    "def activation_patching_coding_analysis(model):\n",
    "    \"\"\"\n",
    "    Runs activation patching experiments on simple coding reasoning templates.\n",
    "    \n",
    "    Analyzes different types of programming logic including:\n",
    "    - Variable assignment and values\n",
    "    - Simple conditionals\n",
    "    - Basic loops\n",
    "    - Function return values\n",
    "    - Array/list operations\n",
    "    \n",
    "    Both clean and corrupted pairs represent valid code with correct outputs,\n",
    "    but demonstrate different programming patterns to achieve the results.\n",
    "    \"\"\"\n",
    "    print(\"Starting activation patching analysis across coding reasoning templates...\")\n",
    "    category = \"coding\"\n",
    "    all_attn_head_outputs = []\n",
    "\n",
    "    # Variable Assignment\n",
    "    template_title = \"VARS\"\n",
    "    base_template = \"x = 5\\ny = {}\\nz = x + y\\nz equals\"\n",
    "    clean_pairs = [\n",
    "        (\"3\", \"8\"),    # Adding small numbers\n",
    "        (\"10\", \"15\"),  # Adding larger numbers\n",
    "        (\"0\", \"5\")     # Adding zero\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"15\", \"20\"),  # Different but valid additions\n",
    "        (\"25\", \"30\"),\n",
    "        (\"20\", \"25\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Conditionals\n",
    "    template_title = \"IF\"\n",
    "    base_template = \"x = {}\\nif x < 10:\\n    print('small')\\nelse:\\n    print('big')\\nThe code will print\"\n",
    "    clean_pairs = [\n",
    "        (\"5\", \"small\"),    # Single digit numbers\n",
    "        (\"3\", \"small\"),\n",
    "        (\"2\", \"small\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"15\", \"big\"),     # Double digit numbers\n",
    "        (\"12\", \"big\"),\n",
    "        (\"11\", \"big\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Loops\n",
    "    template_title = \"LOOP\"\n",
    "    base_template = \"total = 0\\nfor i in range({}):\\n    total += 2\\ntotal equals\"\n",
    "    clean_pairs = [\n",
    "        (\"3\", \"6\"),    # Small ranges\n",
    "        (\"4\", \"8\"),\n",
    "        (\"5\", \"10\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"10\", \"20\"),  # Larger ranges\n",
    "        (\"8\", \"16\"),\n",
    "        (\"6\", \"12\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Function Returns\n",
    "    template_title = \"FUNC\"\n",
    "    base_template = \"def func(x):\\n    return x * {}\\n\\nresult = func(2)\\nresult equals\"\n",
    "    clean_pairs = [\n",
    "        (\"3\", \"6\"),    # Multiplication by small numbers\n",
    "        (\"4\", \"8\"),\n",
    "        (\"5\", \"10\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"10\", \"20\"),  # Multiplication by larger numbers\n",
    "        (\"8\", \"16\"),\n",
    "        (\"6\", \"12\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # List Operations\n",
    "    template_title = \"LIST\"\n",
    "    base_template = \"nums = [1, 2, {}]\\nsum = 0\\nfor n in nums:\\n    sum += n\\nsum equals\"\n",
    "    clean_pairs = [\n",
    "        (\"3\", \"6\"),     # Small numbers in list\n",
    "        (\"4\", \"7\"),\n",
    "        (\"5\", \"8\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"10\", \"13\"),   # Larger numbers in list\n",
    "        (\"15\", \"18\"),\n",
    "        (\"20\", \"23\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Calculate and save the average\n",
    "    avg_attn_head_out = t.stack(all_attn_head_outputs).mean(dim=0)\n",
    "    \n",
    "    # Plot and save the average\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    max_abs_val = abs(avg_attn_head_out.cpu()).max()\n",
    "    plt.imshow(\n",
    "        avg_attn_head_out.cpu(),\n",
    "        cmap=\"RdBu\",\n",
    "        aspect=\"auto\",\n",
    "        vmin=-max_abs_val,\n",
    "        vmax=max_abs_val\n",
    "    )\n",
    "    plt.colorbar(label=\"Value\")\n",
    "    plt.xlabel(\"Head\")\n",
    "    plt.ylabel(\"Layer\")\n",
    "    plt.title(\"Average Attention Head Outputs Across Coding Templates\")\n",
    "    plt.savefig(f\"../results/{model_name}/{category}/activation_patching_attn_head_out_avg.png\")\n",
    "    plt.close()\n",
    "\n",
    "def activation_patching_transitive_analysis(model):\n",
    "    \"\"\"\n",
    "    Analyzes the model's ability to handle multi-step (transitive) reasoning.\n",
    "    Tests different types of transitive relationships:\n",
    "    - Age/Time relationships\n",
    "    - Spatial/Location relationships\n",
    "    - Family relationships\n",
    "    - Comparative relationships\n",
    "    \"\"\"\n",
    "    print(\"Starting activation patching analysis across transitive reasoning templates...\")\n",
    "    category = \"transitive\"\n",
    "    all_attn_head_outputs = []\n",
    "\n",
    "    # # Age relationships\n",
    "    # template_title = \"AGE\"\n",
    "    # base_template = \"Pete is {}. Pete and Andy are the same age. Andy is\"\n",
    "    # clean_pairs = [\n",
    "    #     (\"20\", \"20\"),\n",
    "    #     (\"15\", \"15\"),\n",
    "    #     (\"30\", \"30\")\n",
    "    # ]\n",
    "    # corrupted_pairs = [\n",
    "    #     (\"90\", \"90\"),\n",
    "    #     (\"5\", \"5\"),\n",
    "    #     (\"40\", \"40\")\n",
    "    # ]\n",
    "    # dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    # attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    # all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Location relationships\n",
    "    template_title = \"LOCATION\"\n",
    "    base_template = \"Sara is in the {}. Sara and Tom are in the same place. Tom is in the\"\n",
    "    clean_pairs = [\n",
    "        (\"kitchen\", \"kitchen\"),\n",
    "        (\"library\", \"library\"),\n",
    "        (\"office\", \"office\"),\n",
    "        (\"classroom\", \"classroom\"),\n",
    "        (\"cafeteria\", \"cafeteria\"),\n",
    "        (\"gym\", \"gym\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"garden\", \"garden\"), \n",
    "        (\"park\", \"park\"),\n",
    "        (\"bedroom\", \"bedroom\"),\n",
    "        (\"basement\", \"basement\"),\n",
    "        (\"attic\", \"attic\"),\n",
    "        (\"garage\", \"garage\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    template_title = \"COLOR\"\n",
    "    base_template = \"The box is {}. The box and ball are the same color. The ball is\"\n",
    "    clean_pairs = [\n",
    "        (\"red\", \"red\"),\n",
    "        (\"blue\", \"blue\"),\n",
    "        (\"green\", \"green\"),\n",
    "        (\"yellow\", \"yellow\"),\n",
    "        (\"purple\", \"purple\"),\n",
    "        (\"orange\", \"orange\")\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"white\", \"white\"),\n",
    "        (\"black\", \"black\"),\n",
    "        (\"brown\", \"brown\"),\n",
    "        (\"pink\", \"pink\"),\n",
    "        (\"gray\", \"gray\"),\n",
    "        (\"gold\", \"gold\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    exit()\n",
    "\n",
    "    # Family relationships\n",
    "    # template_title = \"FAMILY\"\n",
    "    # base_template = \"Jane is Alex's {}. Bob is Jane's son. Alex is Bob's\"\n",
    "    # clean_pairs = [\n",
    "    #     (\"sister\", \"aunt\"),\n",
    "    #     (\"mother\", \"sister\"),\n",
    "    # ]\n",
    "    # corrupted_pairs = [\n",
    "    #     (\"aunt\", \"cousin\"),\n",
    "    #     (\"daughter\", \"grandma\")\n",
    "    # ]\n",
    "    # dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    # attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    # all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Time relationships\n",
    "    template_title = \"TIME\"\n",
    "    base_template = \"Event A happened {} Event B. Event B happened before Event C. Event C happened\"\n",
    "    clean_pairs = [\n",
    "        (\"before\", \"last\"),\n",
    "        (\"after\", \"second\"),\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"after\", \"second\"),\n",
    "        (\"before\", \"last\"),\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Cost comparisons\n",
    "    template_title = \"COST\"\n",
    "    base_template = \"The book costs {} than the pen. The pen costs more than the pencil. The pencil is the\"\n",
    "    clean_pairs = [\n",
    "        (\"more\", \"cheapest\"),\n",
    "        (\"less\", \"middle\"),\n",
    "    ]\n",
    "    corrupted_pairs = [\n",
    "        (\"less\", \"middle\"),\n",
    "        (\"more\", \"cheapest\")\n",
    "    ]\n",
    "    dataset = create_patching_dataset(model, clean_pairs, corrupted_pairs, base_template)\n",
    "    attn_head_out = activation_patching(model, category, template_title, dataset)\n",
    "    all_attn_head_outputs.append(attn_head_out)\n",
    "\n",
    "    # Calculate and save the average\n",
    "    avg_attn_head_out = t.stack(all_attn_head_outputs).mean(dim=0)\n",
    "    \n",
    "    # Plot and save the average\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    max_abs_val = abs(avg_attn_head_out.cpu()).max()\n",
    "    plt.imshow(\n",
    "        avg_attn_head_out.cpu(),\n",
    "        cmap=\"RdBu\",\n",
    "        aspect=\"auto\",\n",
    "        vmin=-max_abs_val,\n",
    "        vmax=max_abs_val\n",
    "    )\n",
    "    plt.colorbar(label=\"Value\")\n",
    "    plt.xlabel(\"Head\")\n",
    "    plt.ylabel(\"Layer\")\n",
    "    plt.title(\"Average Attention Head Outputs Across Transitive Templates\")\n",
    "    plt.savefig(f\"../results/{model_name}/{category}/activation_patching_attn_head_out_avg.png\")\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main entry point for the reasoning interpretation analysis.\n",
    "    \n",
    "    Sets up the model and directory structure, then runs various analyses\n",
    "    to understand how the model processes causal relationships.\n",
    "    \"\"\"\n",
    "    global device\n",
    "    device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Set up the model and directory structure\n",
    "    # model_name = \"gpt2-small\"\n",
    "    model_name = \"gpt2-medium\"\n",
    "\n",
    "    print(f\"Initializing analysis with model: {model_name}\")\n",
    "    global directory_name\n",
    "    directory_name = f\"/content/{model_name}\"  # Choose your desired directory name\n",
    "    os.makedirs(os.path.join(directory_name), exist_ok=True)\n",
    "    model: HookedTransformer = HookedTransformer.from_pretrained(\n",
    "        model_name\n",
    "    )\n",
    "    # analyze_delimiter_attention(model)\n",
    "    # analyze_causal_attention(model)\n",
    "    global skip_resid_pre \n",
    "    skip_resid_pre = True\n",
    "    activation_patching_paper_templates_analysis(model)\n",
    "    activation_patching_mathematical_analysis(model)\n",
    "    # activation_patching_emotional_analysis(model)\n",
    "    # activation_patching_physical_analysis(model)\n",
    "    # activation_patching_arithmetic_analysis(model)\n",
    "    # activation_patching_transitive_analysis(model)\n",
    "\n",
    "    # activation_patching_coding_logic_analysis(model)\n",
    "\n",
    "# NOTES:\n",
    "# Max has 4 apples, tom has 6, who has more? Combine math and semantic reasoning\n",
    "# Coding logic!\n",
    "# write code for math type reasoning tasks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
